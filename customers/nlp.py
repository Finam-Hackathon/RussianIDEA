from transformers import pipeline
import torch
import re
from typing import Dict
import logging
import os
import time
import sys
import transformers
import datetime
import hashlib
from functools import lru_cache
from typing import Dict, Optional



# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–¥–æ–±–∞–≤—å—Ç–µ –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞)
_SUMMARIZATION_METRICS = {
    'requests_total': 0,
    'errors_total': 0,
    'processing_times': [],
    'last_reset': datetime.datetime.now()
}


def _get_text_hash(text: str) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ö–µ—à–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
    return hashlib.md5(text.encode()).hexdigest()


@lru_cache(maxsize=100)
def _cached_summarization(text_hash: str, config_key: str, config_params: tuple) -> str:
    """
    –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–µ—à–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    """
    # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –≤—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏
    # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–≥–ª—É—à–∫—É, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∑–∞–º–µ–Ω–µ–Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
    return f"–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–æ–Ω—Ñ–∏–≥–∞ {config_key}"


def _record_metrics(success: bool = True, processing_time: float = 0.0):
    """
    –ó–∞–ø–∏—Å—å –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    global _SUMMARIZATION_METRICS

    _SUMMARIZATION_METRICS['requests_total'] += 1

    if success:
        _SUMMARIZATION_METRICS['processing_times'].append(processing_time)
    else:
        _SUMMARIZATION_METRICS['errors_total'] += 1


# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–ª–∞—Å—Å –∫–∞–∫ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã)
def get_summarization_metrics() -> Dict:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –º–µ—Ç—Ä–∏–∫ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    global _SUMMARIZATION_METRICS

    times = _SUMMARIZATION_METRICS['processing_times']
    total_requests = _SUMMARIZATION_METRICS['requests_total']

    avg_time = sum(times) / len(times) if times else 0
    error_rate = (_SUMMARIZATION_METRICS['errors_total'] / total_requests * 100) if total_requests > 0 else 0

    return {
        'total_requests': total_requests,
        'total_errors': _SUMMARIZATION_METRICS['errors_total'],
        'error_rate_percent': round(error_rate, 2),
        'avg_processing_time_seconds': round(avg_time, 2),
        'performance_benchmark': 'good' if avg_time < 2.0 else 'needs_optimization'
    }


def clear_summarization_cache():
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    _cached_summarization.cache_clear()
    logging.getLogger('RADARFinancialSummarizer').info("üßπ –ö—ç—à —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –æ—á–∏—â–µ–Ω")


def get_cache_info() -> Dict:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ"""
    cache_info = _cached_summarization.cache_info()
    return {
        'cache_hits': cache_info.hits,
        'cache_misses': cache_info.misses,
        'cache_size': cache_info.currsize,
        'cache_max_size': cache_info.maxsize
    }

class RADARFinancialSummarizer:
    """
    –§–ò–ù–ê–õ–¨–ù–ê–Ø –ì–û–¢–û–í–ê–Ø –°–ò–°–¢–ï–ú–ê –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π RADAR
    """

    def __init__(self, device: str = "auto", debug_mode: bool = False):
        self.setup_logging()
        self.device = self._setup_device(device)
        self.model_name = "facebook/bart-large-cnn"
        self.debug_mode = debug_mode  # –î–æ–±–∞–≤—å—Ç–µ —ç—Ç—É —Å—Ç—Ä–æ–∫—É

        self.model_kwargs = {
            'low_cpu_mem_usage': True,
            'torchscript': True,  # üöÄ –£—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            'use_cache': True,  # üìù –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è
        }

        # üéØ –§–ò–ù–ê–õ–¨–ù–´–ï –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò
        self.configs = {
            "EARNINGS": {'max_length': 50, 'min_length': 25, 'length_penalty': 1.6, 'num_beams': 4, 'early_stopping': True, },
            "CENTRAL_BANK": {'max_length': 35, 'min_length': 18, 'length_penalty': 2.1, 'num_beams': 4, 'early_stopping': True,},
            "MARKET": {'max_length': 30, 'min_length': 15, 'length_penalty': 2.0, 'num_beams': 4, 'early_stopping': True, },
            "MERGERS": {'max_length': 70, 'min_length': 35, 'length_penalty': 1.7, 'num_beams': 4, 'early_stopping': True,},
            "REGULATORY": {'max_length': 60, 'min_length': 30, 'length_penalty': 2.0, 'num_beams': 4, 'early_stopping': True,},
            "ECONOMIC": {'max_length': 55, 'min_length': 28, 'length_penalty': 1.9, 'num_beams': 4, 'early_stopping': True,},
            "TECHNOLOGY": {'max_length': 65, 'min_length': 32, 'length_penalty': 1.8, 'num_beams': 4, 'early_stopping': True,},
            "COMMODITIES": {'max_length': 50, 'min_length': 25, 'length_penalty': 2.0, 'num_beams': 4, 'early_stopping': True,},
            "GENERAL_FINANCIAL": {'max_length': 60, 'min_length': 30, 'length_penalty': 2.0, 'num_beams': 4, 'early_stopping': True,},
        }

        self.setup_model()

    def _get_adaptive_config(self, text: str, news_type: str) -> Dict:
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞"""
        base_config = self.configs.get(news_type, self.configs["GENERAL_FINANCIAL"]).copy()

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤
        words = text.split()
        word_count = len(words)

        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ max_length –∏ min_length
        if word_count < 30:
            base_config['max_length'] = max(15, word_count // 2)
            base_config['min_length'] = max(8, word_count // 3)
        elif word_count > 200:
            base_config['max_length'] = min(60, base_config['max_length'])
            base_config['min_length'] = min(30, base_config['min_length'])

        # –î–ª—è CPU —É–º–µ–Ω—å—à–∞–µ–º num_beams –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        if self.device == "cpu":
            base_config['num_beams'] = 2  # –ë—ã—Å—Ç—Ä–µ–µ —á–µ–º 4

        return base_config

    def _setup_device(self, device: str) -> str:
        """
        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
        """
        try:
            if device == "auto":
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: CUDA —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–∞–º—è—Ç–∏
                if torch.cuda.is_available():
                    gpu_name = torch.cuda.get_device_name(0)
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9

                    self.logger.info(f"üéÆ –û–±–Ω–∞—Ä—É–∂–µ–Ω GPU: {gpu_name} ({gpu_memory:.1f}GB)")

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏ –¥–ª—è BART-large
                    if gpu_memory >= 4.0:  # –ú–∏–Ω–∏–º—É–º 4GB –¥–ª—è –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã
                        return "cuda"
                    else:
                        self.logger.warning(f"‚ö†Ô∏è GPU –ø–∞–º—è—Ç—å {gpu_memory:.1f}GB –º–∞–ª–æ–≤–∞—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
                        return "cpu"

                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: Apple Silicon
                elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                    self.logger.info("üçé –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Apple Silicon (MPS)")
                    return "mps"

                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: CPU
                else:
                    cpu_count = os.cpu_count()
                    self.logger.info(f"‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU (—è–¥–µ—Ä: {cpu_count})")
                    return "cpu"

            # –†—É—á–Ω–æ–π –≤—ã–±–æ—Ä —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
            elif device == "cuda":
                if torch.cuda.is_available():
                    return "cuda"
                else:
                    self.logger.warning("‚ùå CUDA –∑–∞–ø—Ä–æ—à–µ–Ω–∞, –Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
                    return "cpu"

            elif device == "mps":
                if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                    return "mps"
                else:
                    self.logger.warning("‚ùå MPS –∑–∞–ø—Ä–æ—à–µ–Ω–∞, –Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
                    return "cpu"

            elif device == "cpu":
                return "cpu"

            else:
                self.logger.warning(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ '{device}', –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
                return "cpu"

        except Exception as e:
            self.logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: {e}")
            return "cpu"  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π fallback

    def setup_logging(self):
        """
        –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Ä–æ—Ç–∞—Ü–∏–µ–π –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        """
        import logging.handlers
        import sys

        self.logger = logging.getLogger('RADARFinancialSummarizer')
        self.logger.setLevel(logging.INFO)

        # –û—á–∏—Å—Ç–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö handlers
        self.logger.handlers.clear()

        # 1. üéØ Console Handler —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)

        class SmartFormatter(logging.Formatter):
            def format(self, record):
                # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Ä–æ–≤–Ω—è
                if record.levelno >= logging.ERROR:
                    record.msg = f"‚ùå {record.msg}"
                elif record.levelno >= logging.WARNING:
                    record.msg = f"‚ö†Ô∏è {record.msg}"
                elif record.levelno >= logging.INFO:
                    record.msg = f"‚ÑπÔ∏è {record.msg}"
                return super().format(record)

        console_formatter = SmartFormatter(
            '%(asctime)s | %(levelname)-8s | [RADAR] %(message)s',
            datefmt='%H:%M:%S'
        )
        console_handler.setFormatter(console_formatter)

        # 2. üíæ Rotating File Handler (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞)
        try:
            os.makedirs('logs', exist_ok=True)

            file_handler = logging.handlers.RotatingFileHandler(
                filename='logs/radar_system.log',
                maxBytes=10 * 1024 * 1024,  # 10MB
                backupCount=5,  # 5 backup —Ñ–∞–π–ª–æ–≤
                encoding='utf-8'
            )
            file_handler.setLevel(logging.DEBUG)  # –í —Ñ–∞–π–ª –ø–∏—à–µ–º –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

            file_formatter = logging.Formatter(
                '%(asctime)s | %(name)-25s | %(levelname)-8s | %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            file_handler.setFormatter(file_formatter)

            self.logger.addHandler(file_handler)

        except Exception as e:
            print(f"‚ö†Ô∏è File logging unavailable: {e}")

        # 3. üìß Error Handler (—Ç–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª)
        try:
            error_handler = logging.FileHandler('logs/radar_errors.log', encoding='utf-8')
            error_handler.setLevel(logging.ERROR)

            error_formatter = logging.Formatter(
                '%(asctime)s | ERROR | %(message)s\nStacktrace: %(exc_info)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            error_handler.setFormatter(error_formatter)

            self.logger.addHandler(error_handler)

        except Exception as e:
            print(f"‚ö†Ô∏è Error logging unavailable: {e}")

        # –î–æ–±–∞–≤–ª—è–µ–º console handler –≤ –∫–æ–Ω—Ü–µ
        self.logger.addHandler(console_handler)

        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
        self.logger.info("=" * 50)
        self.logger.info("üöÄ RADAR Financial Summarizer Started")
        self.logger.info("=" * 50)

    def setup_model(self):
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        """
        try:
            self.logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_name}...")

            # üìä –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
            self._log_system_info()

            # ‚è±Ô∏è –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≥—Ä—É–∑–∫–∏
            start_time = time.time()

            # üéØ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            self.summarizer = pipeline(
                "summarization",
                model=self.model_name,
                tokenizer=self.model_name,
                device=0 if self.device == "cuda" else -1,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,  # üöÄ –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
                trust_remote_code=True,  # üîß –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π
                model_kwargs={
                    'low_cpu_mem_usage': True,  # üìâ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è RAM
                    'force_download': False,  # üíæ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
                    'resume_download': True,  # üîÑ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
                }
            )

            # üìà –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            load_time = time.time() - start_time
            self.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.1f} —Å–µ–∫—É–Ω–¥")

            # üß™ –¢–µ—Å—Ç–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã
            self._test_model_functionality()

            # üíæ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏
            self._log_memory_usage()

        except OSError as e:
            # üåê –û—à–∏–±–∫–∏ —Å–µ—Ç–∏/–∑–∞–≥—Ä—É–∑–∫–∏
            if "404" in str(e):
                self.logger.error(f"‚ùå –ú–æ–¥–µ–ª—å {self.model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ HuggingFace Hub")
                self.logger.info("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
            elif "timeout" in str(e).lower():
                self.logger.error("‚ùå –¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏")
                self.logger.info("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å timeout –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
            else:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise

        except RuntimeError as e:
            # üíª –û—à–∏–±–∫–∏ –ø–∞–º—è—Ç–∏/—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            if "CUDA out of memory" in str(e):
                self.logger.error("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ GPU")
                self.logger.info("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CPU –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç—å batch_size")
            elif "CUDA" in str(e):
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ CUDA: {e}")
                self.logger.info("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥—Ä–∞–π–≤–µ—Ä—ã NVIDIA –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å PyTorch+CUDA")
            else:
                self.logger.error(f"‚ùå Runtime –æ—à–∏–±–∫–∞: {e}")
            raise

        except Exception as e:
            self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            self.logger.info("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É transformers –∏ torch")
            raise

    def _log_system_info(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ"""
        self.logger.info(f"‚öôÔ∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        self.logger.info(f"üêç Python: {sys.version}")
        self.logger.info(f"üî• PyTorch: {torch.__version__}")
        self.logger.info(f"ü§ó Transformers: {transformers.__version__}")

        if self.device == "cuda":
            self.logger.info(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
            self.logger.info(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

    def _test_model_functionality(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        try:
            test_text = "This is a test sentence to verify the model is working correctly."
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∞
            words_count = len(test_text.split())
            max_length = max(15, words_count // 2)
            min_length = max(8, words_count // 3)

            test_result = self.summarizer(test_text, max_length=max_length, min_length=min_length)

            if test_result and len(test_result) > 0:
                self.logger.info("üß™ –¢–µ—Å—Ç–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ")
            else:
                self.logger.warning("‚ö†Ô∏è –¢–µ—Å—Ç–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –¢–µ—Å—Ç–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")

    def _log_memory_usage(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        if self.device == "cuda":
            memory_allocated = torch.cuda.memory_allocated() / 1e6
            memory_reserved = torch.cuda.memory_reserved() / 1e6
            self.logger.info(f"üíæ –ü–∞–º—è—Ç—å GPU: {memory_allocated:.1f}MB / {memory_reserved:.1f}MB")

    def summarize_news(self, text: str, generate_draft: bool = True, use_cache: bool = True) -> Dict:
        """
        –£–õ–£–ß–®–ï–ù–ù–ê–Ø –§–ò–ù–ê–õ–¨–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            generate_draft: –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ª–∏ —á–µ—Ä–Ω–æ–≤–∏–∫
            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ

        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–µ
        """
        start_time = time.time()

        # 1. –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        validation_error = self._validate_summarization_input(text)
        if validation_error:
            _record_metrics(success=False)
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {validation_error}")
            return {"error": validation_error, "error_type": "validation"}

        text = text.strip()
        self.logger.info(f"üöÄ –ù–∞—á–∞—Ç–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

        try:
            # 2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –Ω–æ–≤–æ—Å—Ç–∏
            news_type = self._detect_news_type(text)
            self.logger.debug(f"üì∞ –û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ç–∏–ø –Ω–æ–≤–æ—Å—Ç–∏: {news_type}")

            # –î–õ–Ø –û–¢–õ–ê–î–ö–ò: –ª–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            if self.debug_mode:
                text_lower = text.lower()
                market_terms = ['s&p', 'dow', 'nasdaq', 'index', 'stock market']
                earnings_terms = ['earnings', 'revenue', 'profit', 'quarterly']

                found_market = [term for term in market_terms if term in text_lower]
                found_earnings = [term for term in earnings_terms if term in text_lower]

                self.logger.debug(f"üîç –ù–∞–π–¥–µ–Ω—ã —Ä—ã–Ω–æ—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {found_market}")
                self.logger.debug(f"üîç –ù–∞–π–¥–µ–Ω—ã –æ—Ç—á–µ—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {found_earnings}")

            # 3. –ü–æ–ª—É—á–µ–Ω–∏–µ –ê–î–ê–ü–¢–ò–í–ù–û–ô –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            config = self._get_adaptive_config(text, news_type)

            # 4. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            processed_text = self.preprocess_text(text)

            # 5. –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏–ª–∏ –±–µ–∑)
            cache_info = "disabled"
            if use_cache:
                text_hash = _get_text_hash(processed_text)
                config_key = news_type
                config_params = tuple(config.values())  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ hashable tuple

                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞
                raw_summary = _cached_summarization(text_hash, config_key, config_params)
                cache_info = "hit"

                # –ï—Å–ª–∏ –∫—ç—à –ø—É—Å—Ç–æ–π, –≤—ã–ø–æ–ª–Ω—è–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é
                if raw_summary.startswith("–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"):
                    result = self.summarizer(processed_text, **config)
                    raw_summary = result[0]['summary_text']
                    cache_info = "miss"
            else:
                # –ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞
                result = self.summarizer(processed_text, **config)
                raw_summary = result[0]['summary_text']

            # 6. –û—á–∏—Å—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            cleaned_summary = self._clean_summary(raw_summary)

            # 7. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–Ω–æ–≤–∏–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            draft = ""
            if generate_draft:
                draft = self._generate_draft(cleaned_summary, news_type)

            # 8. –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            stats = self._calculate_stats(text, cleaned_summary)
            stats['cache'] = cache_info
            stats['text_length_original'] = len(text)
            stats['text_length_summary'] = len(cleaned_summary)

            # 9. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–µ—Ç—Ä–∏–∫–∏
            processing_time = time.time() - start_time
            _record_metrics(success=True, processing_time=processing_time)

            self.logger.info(
                f"‚úÖ –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. "
                f"–ö–∞—á–µ—Å—Ç–≤–æ: {stats.get('quality', 'N/A')}, "
                f"–°–∂–∞—Ç–∏–µ: {stats.get('compression_ratio', 0)}, "
                f"–ö—ç—à: {cache_info}, "
                f"–í—Ä–µ–º—è: {processing_time:.2f}—Å"
            )

            return {
                "summary": cleaned_summary,
                "news_type": news_type,
                "draft": draft,
                "stats": stats,
                "processing_time": round(processing_time, 2),
                "success": True
            }

        except ValueError as e:
            # –û—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
            processing_time = time.time() - start_time
            _record_metrics(success=False, processing_time=processing_time)
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {e} [–í—Ä–µ–º—è: {processing_time:.2f}—Å]")
            return {"error": str(e), "error_type": "data_validation"}

        except TimeoutError as e:
            # –¢–∞–π–º–∞—É—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏
            processing_time = time.time() - start_time
            _record_metrics(success=False, processing_time=processing_time)
            self.logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç: {e} [–í—Ä–µ–º—è: {processing_time:.2f}—Å]")
            return {"error": "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏", "error_type": "timeout"}

        except Exception as e:
            # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏
            processing_time = time.time() - start_time
            _record_metrics(success=False, processing_time=processing_time)
            self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e} [–í—Ä–µ–º—è: {processing_time:.2f}—Å]")

            # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –æ—à–∏–±–∫–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            self.logger.exception("–î–µ—Ç–∞–ª–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–π –æ—à–∏–±–∫–∏:")

            return {
                "error": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–∏—Å–∞",
                "error_type": "internal",
                "details": str(e) if self.debug_mode else None  # –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏
            }

    def _validate_summarization_input(self, text: str) -> Optional[str]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        """
        if not text or not isinstance(text, str):
            return "–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π"

        text = text.strip()

        if len(text) < 30:
            return "–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"

        if len(text) > 100000:
            return "–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤)
        words = text.split()
        if len(words) < 5:
            return "–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ö–æ—Ç—è –±—ã 5 —Å–ª–æ–≤"

        return None

    def _detect_news_type(self, text: str) -> str:
        """
        –£–õ–£–ß–®–ï–ù–ù–ê–Ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
        """
        text_lower = text.lower()

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–∞–º—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
        type_checks = [
            # (—Ç–∏–ø, –∫–ª—é—á–µ–≤—ã–µ_—Å–ª–æ–≤–∞, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
            ("MERGERS", ['acquisition', 'merger', 'takeover', 'buyout', 'deal'], 10),
            ("EARNINGS", [
                'earnings', 'revenue', 'profit', 'quarterly', 'q1', 'q2', 'q3', 'q4',
                'financial results', 'beat estimates', 'missed estimates', 'eps', 'ebitda'
            ], 9),
            ("CENTRAL_BANK", [
                'fed', 'federal reserve', 'ecb', 'central bank', 'interest rate',
                'rate hike', 'rate cut', 'monetary policy', 'powell', 'lagarde'
            ], 8),
            ("MARKET", [
                's&p', 'dow', 'nasdaq', 'index', 'stock market', 'trading session',
                'market close', 'intraday', 'points', 'gains', 'losses', 'stock', 'stocks',
                'equities', 'market index', 'indexes', 'trading volume', 'market volatility'
            ], 7),
            ("REGULATORY", ['regulation', 'sec', 'lawsuit', 'legal', 'ftc', 'doj'], 6),
            ("ECONOMIC", ['gdp', 'unemployment', 'cpi', 'economic data', 'jobs report'], 5),
        ]

        # –ù–∞—Ö–æ–¥–∏–º —Ç–∏–ø —Å –Ω–∞–∏–≤—ã—Å—à–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
        best_type = "GENERAL_FINANCIAL"
        best_priority = 0
        best_match_count = 0

        for news_type, keywords, priority in type_checks:
            matches = sum(1 for keyword in keywords if keyword in text_lower)
            if matches > 0:
                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤—ã—à–µ –ò–õ–ò –±–æ–ª—å—à–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø—Ä–∏ —Ä–∞–≤–Ω–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–µ
                if (priority > best_priority) or (priority == best_priority and matches > best_match_count):
                    best_priority = priority
                    best_match_count = matches
                    best_type = news_type

        # –û—Å–æ–±—ã–π —Å–ª—É—á–∞–π: –µ—Å–ª–∏ –µ—Å—Ç—å "earnings" –Ω–æ —Ç–∞–∫–∂–µ –º–Ω–æ–≥–æ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ - —ç—Ç–æ MARKET
        if best_type == "EARNINGS" and any(
                term in text_lower for term in ['s&p', 'dow', 'nasdaq', 'index', 'stock market']):
            market_terms = sum(1 for term in ['s&p', 'dow', 'nasdaq', 'index', 'stock market'] if term in text_lower)
            if market_terms >= 2:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 2 —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–∞
                best_type = "MARKET"

        return best_type

    def preprocess_text(self, text: str) -> str:
        """
        –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        # 1. üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤
        text = re.sub(r'<.*?>', '', text)
        # 2. üìß –£–¥–∞–ª–µ–Ω–∏–µ email
        text = re.sub(r'\S+@\S+', '', text)
        # 3. üîó –£–¥–∞–ª–µ–Ω–∏–µ URL (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞)
        text = re.sub(r'https?://\S+', '', text)
        # 4. üîÑ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text)
        # 5. ‚úÇÔ∏è –û–±—Ä–µ–∑–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤
        text = text.strip()
        # 6. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–∞ –≤ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        text = re.sub(r'([.!?])([–ê-–ØA-Z])', r'\1 \2', text)

        return text

    def _clean_summary(self, summary: str) -> str:
        """
        –£–õ–£–ß–®–ï–ù–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —á–∏—Å–ª–æ–≤—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        """
        # 1. üí∞ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –§–ò–ù–ê–ù–°–û–í–û–ì–û –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–Ø
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–æ—Å–ª–µ —Ç–æ—á–µ–∫ –≤ —á–∏—Å–ª–∞—Ö: $89. 5 ‚Üí $89.5
        summary = re.sub(r'(\$?\d+)\.\s+(\d+)', r'\1.\2', summary)

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—É–º–º
        summary = re.sub(r'\$(\s*)(\d+(?:\.\d+)?)\s*(?:billion|million)?', r'$\2', summary)
        summary = re.sub(r'(\d+(?:\.\d+)?)\s*%', r'\1%', summary)
        summary = re.sub(r'(\d+(?:\.\d+)?)%\s*-\s*(\d+(?:\.\d+)?)%', r'\1%-\2%', summary)

        # 2. üî§ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–π
        company_fixes = {
            'apple': 'Apple', 'fed': 'Fed', 'ceo': 'CEO', 'eps': 'EPS', 'ebitda': 'EBITDA',
            'iphone': 'iPhone', 's&p': 'S&P'
        }

        for wrong, correct in company_fixes.items():
            summary = re.sub(r'\b' + wrong + r'\b', correct, summary, flags=re.IGNORECASE)

        # 3. üìù –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentences = [s.strip() for s in re.split(r'[.!?]+', summary) if s.strip()]

        if sentences:
            cleaned_sentences = []
            for sent in sentences:
                words = sent.split()
                if len(words) >= 2:  # –£–º–µ–Ω—å—à–∏–ª —Å 3 –¥–æ 2 –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                    # –ö–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞
                    if sent and sent[0].isalpha():
                        sent = sent[0].upper() + sent[1:]
                    cleaned_sentences.append(sent)

            summary = '. '.join(cleaned_sentences)

            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–∫–∞
            if summary and summary[-1] not in '.!?':
                summary += '.'

        # 4. üßπ –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        summary = re.sub(r'\s+', ' ', summary).strip()
        summary = re.sub(r'\s([.,!?])', r'\1', summary)  # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–µ—Ä–µ–¥ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π

        return summary

    def _generate_draft(self, summary: str, news_type: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–Ω–æ–≤–∏–∫–∞"""

        templates = {
            "EARNINGS": f"""üìä **EARNINGS REPORT**

    {summary}

    **KEY HIGHLIGHTS:**
    ‚Ä¢ Financial performance metrics
    ‚Ä¢ Growth and profitability trends  
    ‚Ä¢ Capital allocation decisions
    ‚Ä¢ Management outlook and guidance

    **INVESTMENT IMPACT:**
    Expected market reaction and analyst response""",

            "CENTRAL_BANK": f"""üè¶ **FED POLICY UPDATE**

    {summary}

    **POLICY DECISION:**
    ‚Ä¢ Interest rate changes
    ‚Ä¢ Economic assessment
    ‚Ä¢ Forward guidance

    **MARKET IMPLICATIONS:**
    Impact on financial markets""",

            "MARKET": f"""üìà **MARKET ANALYSIS**

    {summary}

    **TODAY'S ACTION:**
    ‚Ä¢ Index performance
    ‚Ä¢ Sector movements  
    ‚Ä¢ Trading activity

    **KEY DRIVERS:**
    Market influences and outlook""",

            "MERGERS": f"""ü§ù **M&A TRANSACTION**

    {summary}

    **DEAL DETAILS:**
    ‚Ä¢ Acquisition terms and valuation
    ‚Ä¢ Strategic rationale and synergies  
    ‚Ä¢ Regulatory approval timeline
    ‚Ä¢ Integration plans and leadership

    **INDUSTRY IMPACT:**
    Competitive landscape changes and market consolidation""",

            "REGULATORY": f"""‚öñÔ∏è **REGULATORY UPDATE**

    {summary}

    **LEGAL DEVELOPMENTS:**
    ‚Ä¢ Regulatory actions and decisions
    ‚Ä¢ Compliance requirements
    ‚Ä¢ Legal proceedings and outcomes
    ‚Ä¢ Policy implications

    **BUSINESS IMPACT:**
    Operational and financial consequences""",

            "ECONOMIC": f"""üìä **ECONOMIC DATA**

    {summary}

    **KEY INDICATORS:**
    ‚Ä¢ Major economic metrics and trends
    ‚Ä¢ Comparison with forecasts and prior periods
    ‚Ä¢ Sector-specific impacts
    ‚Ä¢ Policy implications

    **MARKET REACTION:**
    Financial market responses and outlook""",

            "TECHNOLOGY": f"""üî¨ **TECHNOLOGY NEWS**

    {summary}

    **INNOVATION HIGHLIGHTS:**
    ‚Ä¢ Technological developments and features
    ‚Ä¢ Research and development progress
    ‚Ä¢ Competitive advancements
    ‚Ä¢ Market adoption and potential

    **INVESTMENT POTENTIAL:**
    Growth opportunities and sector impact""",

            "COMMODITIES": f"""üõ¢Ô∏è **COMMODITIES UPDATE**

    {summary}

    **MARKET MOVEMENTS:**
    ‚Ä¢ Price changes and trading patterns
    ‚Ä¢ Supply and demand factors
    ‚Ä¢ Geopolitical influences
    ‚Ä¢ Inventory and production data

    **TRADING OUTLOOK:**
    Price forecasts and risk factors""",

            "GENERAL_FINANCIAL": f"""üì∞ **FINANCIAL NEWS**

    {summary}

    **KEY DEVELOPMENTS:**
    Main events and their financial significance

    **MARKET IMPLICATIONS:**
    Potential impacts and considerations"""
        }

        return templates.get(news_type, templates["GENERAL_FINANCIAL"])

    def _calculate_stats(self, original: str, summary: str) -> Dict:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        orig_words = len(original.split())
        summ_words = len(summary.split())
        orig_chars = len(original)
        summ_chars = len(summary)
        orig_sentences = len([s for s in original.split('.') if s.strip()])
        summ_sentences = len([s for s in summary.split('.') if s.strip()])

        # –†–∞—Å—á–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤
        ratio_words = orig_words / summ_words if summ_words > 0 else 1
        ratio_chars = orig_chars / summ_chars if summ_chars > 0 else 1
        reduction_percent = (1 - summ_words / orig_words) * 100 if orig_words > 0 else 0

        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
        if 2.0 <= ratio_words <= 3.5 and summ_sentences >= 1:
            quality = "‚úÖ –û—Ç–ª–∏—á–Ω–æ"
            quality_score = 5
        elif 1.5 <= ratio_words < 2.0 and summ_sentences >= 1:
            quality = "‚úÖ –•–æ—Ä–æ—à–æ"
            quality_score = 4
        elif ratio_words > 3.5:
            quality = "‚ö†Ô∏è –ú–æ–∂–Ω–æ –∫–æ—Ä–æ—á–µ"
            quality_score = 3
        elif ratio_words < 1.5:
            quality = "‚ö†Ô∏è –ú–æ–∂–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–µ–µ"
            quality_score = 2
        else:
            quality = "‚ùå –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ"
            quality_score = 1

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        words_per_sentence = summ_words / summ_sentences if summ_sentences > 0 else 0
        density_score = summ_words / orig_words if orig_words > 0 else 0

        # –ê–Ω–∞–ª–∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
        unique_words_ratio = len(set(summary.split())) / summ_words if summ_words > 0 else 0
        info_density = "–í—ã—Å–æ–∫–∞—è" if unique_words_ratio > 0.7 else "–°—Ä–µ–¥–Ω—è—è" if unique_words_ratio > 0.5 else "–ù–∏–∑–∫–∞—è"

        return {
            # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            "original_words": orig_words,
            "summary_words": summ_words,
            "original_chars": orig_chars,
            "summary_chars": summ_chars,
            "original_sentences": orig_sentences,
            "summary_sentences": summ_sentences,

            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã —Å–∂–∞—Ç–∏—è
            "compression_ratio_words": f"{ratio_words:.1f}x",
            "compression_ratio_chars": f"{ratio_chars:.1f}x",
            "reduction_percent": f"{reduction_percent:.0f}%",
            "compression_ratio": f"{ratio_words:.1f}x",

            # –ö–∞—á–µ—Å—Ç–≤–æ –∏ –æ—Ü–µ–Ω–∫–∞
            "quality": quality,
            "quality_score": quality_score,  # —á–∏—Å–ª–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –æ—Ç 1 –¥–æ 5
            "info_density": info_density,  # –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            "words_per_sentence": round(words_per_sentence, 1),
            "density_score": round(density_score, 3),
            "unique_words_ratio": f"{unique_words_ratio:.1%}",

            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            "recommendation": self._generate_recommendation(ratio_words, summ_sentences, quality_score)
        }

    def _generate_recommendation(self, ratio: float, sentences: int, score: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        recommendations = []

        if ratio > 4.0:
            recommendations.append("—É–≤–µ–ª–∏—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é")
        elif ratio < 1.2:
            recommendations.append("—Å–æ–∫—Ä–∞—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç")

        if sentences < 1:
            recommendations.append("–¥–æ–±–∞–≤–∏—Ç—å –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è")
        elif sentences > 5:
            recommendations.append("–æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è")

        if score < 3:
            recommendations.append("–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å")

        return "; ".join(recommendations) if recommendations else "–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ"


# üéØ –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢
def test_radar():
    """–§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –≥–æ—Ç–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã RADAR"""

    test_articles = [
        {
            "text": """Apple Inc. reported fourth-quarter revenue of $89.5 billion, beating analyst estimates of $88.9 billion. 
            iPhone sales grew 12% year-over-year to $42.3 billion, while services revenue reached an all-time high of $19.2 billion. 
            The company announced a new $100 billion share buyback program and increased its dividend by 5% to $0.24 per share. 
            CEO Tim Cook stated that the company is seeing strong growth in emerging markets and expects continued momentum.""",
            "expected_type": "EARNINGS"
        },
        {
            "text": """The Federal Reserve maintained its benchmark interest rate at 5.25%-5.50% during today's policy meeting. 
            Fed Chair Jerome Powell emphasized that the central bank remains committed to bringing inflation down to its 2% target. 
            The updated economic projections show most officials expect at least one more rate hike this year.""",
            "expected_type": "CENTRAL_BANK"
        },
        {
            "text": """The S&P 500 index rose 1.5% today, led by technology stocks amid positive earnings reports. 
            Trading volume was above average as investors reacted to the Fed's policy decision and economic data. 
            Market volatility declined as uncertainty about interest rates diminished. The Dow Jones gained 200 points.""",
            "expected_type": "MARKET"
        },
        # –î–æ–±–∞–≤—å—Ç–µ —Ç–µ—Å—Ç –Ω–∞ –æ—à–∏–±–∫–∏
        {
            "text": "Short text",
            "expected_type": "ERROR",
            "description": "–¢–µ—Å—Ç –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"
        },
        {
            "text": "",
            "expected_type": "ERROR",
            "description": "–¢–µ—Å—Ç –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"
        }
    ]

    print("üöÄ RADAR - –§–ò–ù–ê–õ–¨–ù–ê–Ø –ì–û–¢–û–í–ê–Ø –°–ò–°–¢–ï–ú–ê")
    print("=" * 60)

    # üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    total_tests = len(test_articles)
    passed_tests = 0
    total_processing_time = 0

    summarizer = RADARFinancialSummarizer()

    for i, test_case in enumerate(test_articles, 1):
        article = test_case["text"]
        expected_type = test_case.get("expected_type")
        description = test_case.get("description", f"–¢–µ—Å—Ç #{i}")

        print(f"\nüß™ –¢–ï–°–¢ #{i}: {description}")
        print("-" * 50)

        start_time = time.time()
        result = summarizer.summarize_news(article)
        processing_time = time.time() - start_time
        total_processing_time += processing_time

        if "error" not in result:
            # ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–µ—Å–ª–∏ –æ–∂–∏–¥–∞–µ–º—ã–π —Ç–∏–ø —É–∫–∞–∑–∞–Ω)
            type_match = "‚ö°"
            if expected_type and expected_type != "ERROR":
                type_match = "‚úÖ" if result['news_type'] == expected_type else "‚ùå"

            print(f"{type_match} –¢–∏–ø: {result['news_type']} (–æ–∂–∏–¥–∞–ª—Å—è: {expected_type})")
            print(f"üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: {result['summary']}")

            # üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats = result['stats']
            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"   ‚Ä¢ –°–ª–æ–≤: {stats['original_words']} ‚Üí {stats['summary_words']}")
            print(f"   ‚Ä¢ –°–∂–∞—Ç–∏–µ: {stats['compression_ratio_words']} ({stats['reduction_percent']})")
            print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ: {stats['quality']}")
            print(f"   ‚Ä¢ –í—Ä–µ–º—è: {processing_time:.2f}—Å")

            if 'summary_sentences' in stats:
                print(f"   ‚Ä¢ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {stats['summary_sentences']}")

            # üìÑ –ß–µ—Ä–Ω–æ–≤–∏–∫ (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ)
            if result['draft']:
                draft_preview = result['draft'][:150] + "..." if len(result['draft']) > 150 else result['draft']
                print(f"üìÑ –ß–µ—Ä–Ω–æ–≤–∏–∫: {draft_preview}")

            passed_tests += 1

        else:
            print(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")
            if expected_type == "ERROR":
                print("‚úÖ –û–∂–∏–¥–∞–µ–º–∞—è –æ—à–∏–±–∫–∞ - —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω")
                passed_tests += 1

        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.2f}—Å")

    # üìà –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 60)
    print("üìä –ò–¢–û–ì–ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
    print(f"‚úÖ –ü—Ä–æ–π–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤: {passed_tests}/{total_tests}")
    print(f"üìà –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {(passed_tests / total_tests) * 100:.1f}%")
    print(f"‚è±Ô∏è –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {total_processing_time / total_tests:.2f}—Å")

    # üéØ –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print(f"\nüéØ –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:")
    metrics = get_summarization_metrics()
    print(f"‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {metrics['total_requests']}")
    print(f"‚Ä¢ –û—à–∏–±–æ–∫: {metrics['total_errors']}")
    print(f"‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {metrics['avg_processing_time_seconds']}—Å")

    # üíæ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—ç—à–µ (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
    try:
        cache_info = get_cache_info()
        print(f"üíæ –ö—ç—à: {cache_info['cache_hits']} –ø–æ–ø–∞–¥–∞–Ω–∏–π, {cache_info['cache_misses']} –ø—Ä–æ–º–∞—Ö–æ–≤")
    except:
        pass

    # üß™ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã
    print(f"\nüîç –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –¢–ï–°–¢–´:")

    # –¢–µ—Å—Ç –Ω–∞ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    long_text = " ".join(["This is a test sentence."] * 50)
    long_result = summarizer.summarize_news(long_text)
    print(f"‚Ä¢ –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {'‚úÖ' if 'error' not in long_result else '‚ùå'}")

    # –¢–µ—Å—Ç –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    special_text = "Apple's revenue grew 15% to $100M - amazing results! #investing"
    special_result = summarizer.summarize_news(special_text)
    print(f"‚Ä¢ –°–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã: {'‚úÖ' if 'error' not in special_result else '‚ùå'}")

    print("üéâ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


# üöÄ –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
def quick_test():
    """–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã"""
    print("‚ö° –ë–´–°–¢–†–´–ô –¢–ï–°–¢ RADAR")

    summarizer = RADARFinancialSummarizer()

    test_text = """Microsoft reported strong quarterly results with cloud revenue growing 25%. 
    The company announced a new AI partnership and increased its dividend."""

    result = summarizer.summarize_news(test_text)

    if "error" not in result:
        print(f"‚úÖ –¢–∏–ø: {result['news_type']}")
        print(f"‚úÖ –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: {result['summary']}")
        print(f"‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ: {result['stats']['quality']}")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")


if __name__ == "__main__":
    # –ú–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –Ω—É–∂–Ω—ã–π —Ç–µ—Å—Ç
    test_radar()  # –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç
